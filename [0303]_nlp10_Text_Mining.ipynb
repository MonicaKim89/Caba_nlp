{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "[0303] nlp10_Text_Mining",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN7cEHYdY36FykN/oALEkKD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MonicaKim89/Caba_nlp/blob/main/%5B0303%5D_nlp10_Text_Mining.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AeoQimpi6n17",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8669dd34-79c9-4a33-a7d4-a7c4c9e9c5bb"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MSKBhz4SDXx3",
        "outputId": "b5756bb9-f3cf-496b-dac0-38fda2a33885"
      },
      "source": [
        "#문장 토큰화 (sent tokenize): 마침표, 개행문자(\\n), 정규표현식\n",
        "from nltk import sent_tokenize\n",
        "text_sample = 'The Matrix is everywhere. It is all around us. Even now, in this very room. You can see it when you look out your window or when you turn on your television. You can feel it when you go to work, when you go to church, when you pay your taxes.'\n",
        "sentences = sent_tokenize(text=text_sample)\n",
        "print(sentences)\n",
        "print(type(sentences), len(sentences))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['The Matrix is everywhere.', 'It is all around us.', 'Even now, in this very room.', 'You can see it when you look out your window or when you turn on your television.', 'You can feel it when you go to work, when you go to church, when you pay your taxes.']\n",
            "<class 'list'> 5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9BcHUnIlEfVV",
        "outputId": "88a1ace6-6901-4c81-e908-1c75017398b6"
      },
      "source": [
        "#단어 토큰화(word_tokenize): 공백, 콤마, 마침표, 개행문자, 정규표현식\n",
        "from nltk import word_tokenize\n",
        "\n",
        "sentence = 'The Matrix is everywhere its all around us, here even in this room.'\n",
        "words = word_tokenize(sentence)\n",
        "print(words)\n",
        "print(type(words), len(words))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.']\n",
            "<class 'list'> 15\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oVq_Z5qfFTfJ",
        "outputId": "4646aa4d-2b35-4399-8c11-3b9138e826e7"
      },
      "source": [
        "#문서에 대해서 모든 단어를 토큰화\n",
        "from nltk import word_tokenize, sent_tokenize\n",
        "\n",
        "def tokenize_text(text):\n",
        "  sentences = sent_tokenize(text)\n",
        "  word_tokens = [word_tokenize(sentence) for sentences in sentences]\n",
        "  return word_tokens\n",
        "\n",
        "word_tokens = tokenize_text(text_sample)\n",
        "print(word_tokens)\n",
        "print(type(word_tokens), len(word_tokens))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.'], ['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.'], ['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.'], ['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.'], ['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.']]\n",
            "<class 'list'> 5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jZfaqWvEHISw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5f78935-28e2-4833-9bd2-1f472a7d434a"
      },
      "source": [
        "#스톱워드 제거: the, is, a, will과 같이 문맥적으로 큰 의미가 없는 단어를 제거\n",
        "import nltk\n",
        "nltk.download('stopwords')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E7pDGHiEIE1q",
        "outputId": "5609ea27-468d-4b17-9792-3862747161a8"
      },
      "source": [
        "#NLTK english stopwords 갯수 확인\n",
        "print(len(nltk.corpus.stopwords.words('english')))\n",
        "print(nltk.corpus.stopwords.words('english')[:20])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "179\n",
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H9B_-6wgIUnM",
        "outputId": "394a7d0f-244d-45a5-807d-182027ad71ad"
      },
      "source": [
        "#stopwords 필터링을 통한 제거\n",
        "import nltk\n",
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "all_tokens = []\n",
        "for sentences in word_tokens:\n",
        "  filtered_words=[]\n",
        "  for word in sentence:\n",
        "    word = word.lower()\n",
        "    if word not in stopwords:\n",
        "      filtered_words.append(word)\n",
        "  all_tokens.append(filtered_words)\n",
        "print(all_tokens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['h', 'e', ' ', 'r', 'x', ' ', ' ', 'e', 'v', 'e', 'r', 'w', 'h', 'e', 'r', 'e', ' ', ' ', 'l', 'l', ' ', 'r', 'u', 'n', ' ', 'u', ',', ' ', 'h', 'e', 'r', 'e', ' ', 'e', 'v', 'e', 'n', ' ', 'n', ' ', 'h', ' ', 'r', '.'], ['h', 'e', ' ', 'r', 'x', ' ', ' ', 'e', 'v', 'e', 'r', 'w', 'h', 'e', 'r', 'e', ' ', ' ', 'l', 'l', ' ', 'r', 'u', 'n', ' ', 'u', ',', ' ', 'h', 'e', 'r', 'e', ' ', 'e', 'v', 'e', 'n', ' ', 'n', ' ', 'h', ' ', 'r', '.'], ['h', 'e', ' ', 'r', 'x', ' ', ' ', 'e', 'v', 'e', 'r', 'w', 'h', 'e', 'r', 'e', ' ', ' ', 'l', 'l', ' ', 'r', 'u', 'n', ' ', 'u', ',', ' ', 'h', 'e', 'r', 'e', ' ', 'e', 'v', 'e', 'n', ' ', 'n', ' ', 'h', ' ', 'r', '.'], ['h', 'e', ' ', 'r', 'x', ' ', ' ', 'e', 'v', 'e', 'r', 'w', 'h', 'e', 'r', 'e', ' ', ' ', 'l', 'l', ' ', 'r', 'u', 'n', ' ', 'u', ',', ' ', 'h', 'e', 'r', 'e', ' ', 'e', 'v', 'e', 'n', ' ', 'n', ' ', 'h', ' ', 'r', '.'], ['h', 'e', ' ', 'r', 'x', ' ', ' ', 'e', 'v', 'e', 'r', 'w', 'h', 'e', 'r', 'e', ' ', ' ', 'l', 'l', ' ', 'r', 'u', 'n', ' ', 'u', ',', ' ', 'h', 'e', 'r', 'e', ' ', 'e', 'v', 'e', 'n', ' ', 'n', ' ', 'h', ' ', 'r', '.']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2BDi9a_IMoke",
        "outputId": "f396426c-4a25-47e6-ee43-a4c8b338fc8d"
      },
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "phTw5oGqJ6Xk",
        "outputId": "0b31a4b1-7ba0-4f1b-9462-00e224f2bc34"
      },
      "source": [
        "#문법적 또는 의미적으로 변화하는 단어의 원형을 찾는 방법\n",
        "#Stemmer(LancasterStemmer)\n",
        "\n",
        "from nltk.stem import LancasterStemmer\n",
        "stemmer = LancasterStemmer()\n",
        "print(stemmer.stem('working'),stemmer.stem('works'),stemmer.stem('worked'))\n",
        "print(stemmer.stem('amusing'),stemmer.stem('aumses'),stemmer.stem('amused'))\n",
        "print(stemmer.stem('fancier'),stemmer.stem('fanciest'))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "work work work\n",
            "amus aums amus\n",
            "fant fanciest\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SKfUXa4VK25j",
        "outputId": "7e2305b9-6ef9-42b3-f4a1-eb81be6025b6"
      },
      "source": [
        "#위에꺼 보다 더 좋은거 Lemmatization (단어를 기본형으로 저장), 정확한 원형 단어 추출을 위해 단어의 품사를 직접 입력\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "\n",
        "lemma = WordNetLemmatizer()\n",
        "print(lemma.lemmatize('working', 'v'),lemma.lemmatize('works','v'),lemma.lemmatize('worked','v'))\n",
        "print(lemma.lemmatize('amusing','v'),lemma.lemmatize('aumses','v'),lemma.lemmatize('amused','v'))\n",
        "print(lemma.lemmatize('fancier'),lemma.lemmatize('fanciest'))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "work work work\n",
            "amuse aumses amuse\n",
            "fancier fanciest\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NtzLWkM5Lzbu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}