{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "[0303] nlp10_Text_Mining",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP5eoKhsyDPWOAKHc7PwN1C",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MonicaKim89/Caba_nlp/blob/main/%5B0303%5D_nlp10_Text_Mining.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJqeITSeN9MX"
      },
      "source": [
        "NLP, 텍스트 분석  \n",
        "Natural Language Processing : 기계가 인간의 언어를 이해하고 해석하는데 중점. 기계번역, 질의응답시스템  \n",
        "텍스트 분석 : 비정형 텍스트에서 의미있는 정보를 추출하는 것에 중점\n",
        "NLP는 텍스트 분석을 향상하게 하는 기반 기술  \n",
        "NLP와 텍스트 분석의 근간에는 머신러닝이 존재. 과거 언어적인 룰 기반 시스템에서 텍스트 데이터 기반으로 모델을 학습하고 예측\n",
        "텍스트 분석은 머신러닝, 언어 이해, 통계 등을 활용한 모델 수립, 정보 추출을 통해 인사이트 및 예측 분석 등의 분석 작업 수행  \n",
        "텍스트 분류 : 신문기사 카테고리 분류, 스팸 메일 검출 프로그램. 지도학습  \n",
        "감성 분석 : 감정/판단/믿음/의견/기분 등의 주관적 요소 분석. 소셜미디어 감정분석, 영화 리뷰, 여론조사 의견분석. 지도학습, 비지도학습  \n",
        "텍스트 요약 : 텍스트 내에서 중요한 주제나 중심 사상을 추출. 토픽 모델링  \n",
        "텍스트 군집화와 유사도 측정 : 비슷한 유형의 문서에 대해 군집화 수행. 비지도 학습  \n",
        "Text 분석 수행 프로세스  \n",
        "텍스트 정규화\n",
        "클랜징, 토큰화, 필터링/스톱워드 제거/철자 수정, Stemming, Lemmatization\n",
        "피처 벡터화 변환\n",
        "Bag of Words : Count 기반, TF-IDF 기반\n",
        "Word2Vec\n",
        "ML 모델 수립 및 학습/예측/평가\n",
        "텍스트 전처리 - 텍스트 정규화\n",
        "클렌징 : 분석에 방해되는 불필요한 문자, 기호를 사전에 제거. HTML, XML 태그나 특정 기호\n",
        "토큰화 : 문서에서 문장을 분리하는 문장 토큰화와 문장에서 단어를 토큰으로 분리하는 단어 토큰화\n",
        "필터링/스톱워드 제거/철자 수정 : 분석에 큰 의미가 없는 단어를 제거\n",
        "Stemming, Lemmatization : 문법적 또는 의미적으로 변화하는 단어의 원형을 찾음\n",
        "Stemming은 원형 단어로 변환 시 일반적인 방법을 적용하거나 더 단순화된 방법을 적용\n",
        "Lemmatization이 Stemming 보다 정교하며 의미론적인 기반에서 단어의 원형을 찾음"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-dV9-xLjN6Nb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AeoQimpi6n17",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8669dd34-79c9-4a33-a7d4-a7c4c9e9c5bb"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MSKBhz4SDXx3",
        "outputId": "b5756bb9-f3cf-496b-dac0-38fda2a33885"
      },
      "source": [
        "#문장 토큰화 (sent tokenize): 마침표, 개행문자(\\n), 정규표현식\n",
        "from nltk import sent_tokenize\n",
        "text_sample = 'The Matrix is everywhere. It is all around us. Even now, in this very room. You can see it when you look out your window or when you turn on your television. You can feel it when you go to work, when you go to church, when you pay your taxes.'\n",
        "sentences = sent_tokenize(text=text_sample)\n",
        "print(sentences)\n",
        "print(type(sentences), len(sentences))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['The Matrix is everywhere.', 'It is all around us.', 'Even now, in this very room.', 'You can see it when you look out your window or when you turn on your television.', 'You can feel it when you go to work, when you go to church, when you pay your taxes.']\n",
            "<class 'list'> 5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9BcHUnIlEfVV",
        "outputId": "88a1ace6-6901-4c81-e908-1c75017398b6"
      },
      "source": [
        "#단어 토큰화(word_tokenize): 공백, 콤마, 마침표, 개행문자, 정규표현식\n",
        "from nltk import word_tokenize\n",
        "\n",
        "sentence = 'The Matrix is everywhere its all around us, here even in this room.'\n",
        "words = word_tokenize(sentence)\n",
        "print(words)\n",
        "print(type(words), len(words))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.']\n",
            "<class 'list'> 15\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oVq_Z5qfFTfJ",
        "outputId": "4646aa4d-2b35-4399-8c11-3b9138e826e7"
      },
      "source": [
        "#문서에 대해서 모든 단어를 토큰화\n",
        "from nltk import word_tokenize, sent_tokenize\n",
        "\n",
        "def tokenize_text(text):\n",
        "  sentences = sent_tokenize(text)\n",
        "  word_tokens = [word_tokenize(sentence) for sentences in sentences]\n",
        "  return word_tokens\n",
        "\n",
        "word_tokens = tokenize_text(text_sample)\n",
        "print(word_tokens)\n",
        "print(type(word_tokens), len(word_tokens))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.'], ['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.'], ['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.'], ['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.'], ['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.']]\n",
            "<class 'list'> 5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jZfaqWvEHISw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5f78935-28e2-4833-9bd2-1f472a7d434a"
      },
      "source": [
        "#스톱워드 제거: the, is, a, will과 같이 문맥적으로 큰 의미가 없는 단어를 제거\n",
        "import nltk\n",
        "nltk.download('stopwords')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E7pDGHiEIE1q",
        "outputId": "5609ea27-468d-4b17-9792-3862747161a8"
      },
      "source": [
        "#NLTK english stopwords 갯수 확인\n",
        "print(len(nltk.corpus.stopwords.words('english')))\n",
        "print(nltk.corpus.stopwords.words('english')[:20])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "179\n",
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H9B_-6wgIUnM",
        "outputId": "394a7d0f-244d-45a5-807d-182027ad71ad"
      },
      "source": [
        "#stopwords 필터링을 통한 제거\n",
        "import nltk\n",
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "all_tokens = []\n",
        "for sentences in word_tokens:\n",
        "  filtered_words=[]\n",
        "  for word in sentence:\n",
        "    word = word.lower()\n",
        "    if word not in stopwords:\n",
        "      filtered_words.append(word)\n",
        "  all_tokens.append(filtered_words)\n",
        "print(all_tokens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['h', 'e', ' ', 'r', 'x', ' ', ' ', 'e', 'v', 'e', 'r', 'w', 'h', 'e', 'r', 'e', ' ', ' ', 'l', 'l', ' ', 'r', 'u', 'n', ' ', 'u', ',', ' ', 'h', 'e', 'r', 'e', ' ', 'e', 'v', 'e', 'n', ' ', 'n', ' ', 'h', ' ', 'r', '.'], ['h', 'e', ' ', 'r', 'x', ' ', ' ', 'e', 'v', 'e', 'r', 'w', 'h', 'e', 'r', 'e', ' ', ' ', 'l', 'l', ' ', 'r', 'u', 'n', ' ', 'u', ',', ' ', 'h', 'e', 'r', 'e', ' ', 'e', 'v', 'e', 'n', ' ', 'n', ' ', 'h', ' ', 'r', '.'], ['h', 'e', ' ', 'r', 'x', ' ', ' ', 'e', 'v', 'e', 'r', 'w', 'h', 'e', 'r', 'e', ' ', ' ', 'l', 'l', ' ', 'r', 'u', 'n', ' ', 'u', ',', ' ', 'h', 'e', 'r', 'e', ' ', 'e', 'v', 'e', 'n', ' ', 'n', ' ', 'h', ' ', 'r', '.'], ['h', 'e', ' ', 'r', 'x', ' ', ' ', 'e', 'v', 'e', 'r', 'w', 'h', 'e', 'r', 'e', ' ', ' ', 'l', 'l', ' ', 'r', 'u', 'n', ' ', 'u', ',', ' ', 'h', 'e', 'r', 'e', ' ', 'e', 'v', 'e', 'n', ' ', 'n', ' ', 'h', ' ', 'r', '.'], ['h', 'e', ' ', 'r', 'x', ' ', ' ', 'e', 'v', 'e', 'r', 'w', 'h', 'e', 'r', 'e', ' ', ' ', 'l', 'l', ' ', 'r', 'u', 'n', ' ', 'u', ',', ' ', 'h', 'e', 'r', 'e', ' ', 'e', 'v', 'e', 'n', ' ', 'n', ' ', 'h', ' ', 'r', '.']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2BDi9a_IMoke",
        "outputId": "f396426c-4a25-47e6-ee43-a4c8b338fc8d"
      },
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "phTw5oGqJ6Xk",
        "outputId": "0b31a4b1-7ba0-4f1b-9462-00e224f2bc34"
      },
      "source": [
        "#문법적 또는 의미적으로 변화하는 단어의 원형을 찾는 방법\n",
        "#Stemmer(LancasterStemmer)\n",
        "\n",
        "from nltk.stem import LancasterStemmer\n",
        "stemmer = LancasterStemmer()\n",
        "print(stemmer.stem('working'),stemmer.stem('works'),stemmer.stem('worked'))\n",
        "print(stemmer.stem('amusing'),stemmer.stem('aumses'),stemmer.stem('amused'))\n",
        "print(stemmer.stem('fancier'),stemmer.stem('fanciest'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "work work work\n",
            "amus aums amus\n",
            "fant fanciest\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SKfUXa4VK25j",
        "outputId": "7e2305b9-6ef9-42b3-f4a1-eb81be6025b6"
      },
      "source": [
        "#위에꺼 보다 더 좋은거 Lemmatization (단어를 기본형으로 저장), 정확한 원형 단어 추출을 위해 단어의 품사를 직접 입력\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "\n",
        "lemma = WordNetLemmatizer()\n",
        "print(lemma.lemmatize('working', 'v'),lemma.lemmatize('works','v'),lemma.lemmatize('worked','v'))\n",
        "print(lemma.lemmatize('amusing','v'),lemma.lemmatize('aumses','v'),lemma.lemmatize('amused','v'))\n",
        "print(lemma.lemmatize('fancier'),lemma.lemmatize('fanciest'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "work work work\n",
            "amuse aumses amuse\n",
            "fancier fanciest\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONh9aAuflfJn"
      },
      "source": [
        "GPU vs CPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NtzLWkM5Lzbu"
      },
      "source": [
        "ㅑ"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BvUIWOYslj7j",
        "outputId": "4488451c-b431-4f67-d167-f7d1868d6c7d"
      },
      "source": [
        "import numpy as np\n",
        "num_samples = 100\n",
        "height = 71\n",
        "width = 71\n",
        "num_classes = 100\n",
        "\n",
        "import tensorflow as tf\n",
        "from keras.applications import Xception\n",
        "import datetime\n",
        "start = datetime.datetime.now()\n",
        "\n",
        "model = Xception(weights = None, \n",
        "                 input_shape = (height, width,3),\n",
        "                 classes= num_classes)\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer = 'rmsprop')\n",
        "x=np.random.random((num_samples,height,width,3))\n",
        "y=np.random.random((num_samples,num_classes))\n",
        "\n",
        "model.fit(x,y,epochs=3, batch_size =16)\n",
        "model.save('my_model.h5')\n",
        "end =datetime.datetime.now()\n",
        "time_delta = end - start\n",
        "\n",
        "print(time_delta)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "7/7 [==============================] - 7s 60ms/step - loss: 233.4736\n",
            "Epoch 2/3\n",
            "7/7 [==============================] - 0s 52ms/step - loss: 241.0297\n",
            "Epoch 3/3\n",
            "7/7 [==============================] - 0s 50ms/step - loss: 240.2627\n",
            "0:00:09.036793\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tyuZLO1Umk0X",
        "outputId": "693492e7-c3e0-4c7b-a9e3-731760994381"
      },
      "source": [
        "import numpy as np\n",
        "num_samples = 100\n",
        "height = 71\n",
        "width = 71\n",
        "num_classes = 100\n",
        "\n",
        "import tensorflow as tf\n",
        "from keras.applications import Xception\n",
        "import datetime\n",
        "start = datetime.datetime.now()\n",
        "#CPU\n",
        "with tf.device('/cpu:0'):\n",
        "\n",
        "  model = Xception(weights = None, \n",
        "                  input_shape = (height, width,3),\n",
        "                  classes= num_classes)\n",
        "  model.compile(loss='categorical_crossentropy',\n",
        "                optimizer = 'rmsprop')\n",
        "  x=np.random.random((num_samples,height,width,3))\n",
        "  y=np.random.random((num_samples,num_classes))\n",
        "\n",
        "  model.fit(x,y,epochs=3, batch_size =16)\n",
        "  model.save('my_model.h5')\n",
        "end =datetime.datetime.now()\n",
        "time_delta = end - start\n",
        "\n",
        "print('cpu', time_delta)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "7/7 [==============================] - 17s 1s/step - loss: 233.8091\n",
            "Epoch 2/3\n",
            "7/7 [==============================] - 10s 1s/step - loss: 242.1351\n",
            "Epoch 3/3\n",
            "7/7 [==============================] - 10s 1s/step - loss: 242.6312\n",
            "cpu 0:00:38.979896\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VskRP1JKoGsY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}